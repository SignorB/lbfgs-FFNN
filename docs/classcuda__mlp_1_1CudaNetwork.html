<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: cuda_mlp::CudaNetwork Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacecuda__mlp.html">cuda_mlp</a></li><li class="navelem"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html">CudaNetwork</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classcuda__mlp_1_1CudaNetwork-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">cuda_mlp::CudaNetwork Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Feed-forward dense network with GPU-backed parameters and gradients.  
 <a href="classcuda__mlp_1_1CudaNetwork.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a81c0dd6e5d78f8dc822d4e1046da8258"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a81c0dd6e5d78f8dc822d4e1046da8258">CudaNetwork</a> (<a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;handle)</td></tr>
<tr class="memdesc:a81c0dd6e5d78f8dc822d4e1046da8258"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct a network tied to a cuBLAS handle.  <a href="classcuda__mlp_1_1CudaNetwork.html#a81c0dd6e5d78f8dc822d4e1046da8258">More...</a><br /></td></tr>
<tr class="separator:a81c0dd6e5d78f8dc822d4e1046da8258"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6dfe3c5761ad5cb3293e1510eebdb971"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a6dfe3c5761ad5cb3293e1510eebdb971">addLayer</a> (int in, int out, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a> act)</td></tr>
<tr class="memdesc:a6dfe3c5761ad5cb3293e1510eebdb971"><td class="mdescLeft">&#160;</td><td class="mdescRight">Append a layer definition.  <a href="classcuda__mlp_1_1CudaNetwork.html#a6dfe3c5761ad5cb3293e1510eebdb971">More...</a><br /></td></tr>
<tr class="separator:a6dfe3c5761ad5cb3293e1510eebdb971"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb5df95bf3d986b04277996b6c32f9ef"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#aeb5df95bf3d986b04277996b6c32f9ef">bindParams</a> (unsigned int seed=2341)</td></tr>
<tr class="memdesc:aeb5df95bf3d986b04277996b6c32f9ef"><td class="mdescLeft">&#160;</td><td class="mdescRight">Allocate parameter/gradient buffers and initialize weights.  <a href="classcuda__mlp_1_1CudaNetwork.html#aeb5df95bf3d986b04277996b6c32f9ef">More...</a><br /></td></tr>
<tr class="separator:aeb5df95bf3d986b04277996b6c32f9ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0ba4e8be8ba2f83046da28cd9a19830c"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a0ba4e8be8ba2f83046da28cd9a19830c">params_size</a> () const</td></tr>
<tr class="memdesc:a0ba4e8be8ba2f83046da28cd9a19830c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Total number of parameters.  <a href="classcuda__mlp_1_1CudaNetwork.html#a0ba4e8be8ba2f83046da28cd9a19830c">More...</a><br /></td></tr>
<tr class="separator:a0ba4e8be8ba2f83046da28cd9a19830c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a5e60cf35eb213d9d704d864d7e6374"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a4a5e60cf35eb213d9d704d864d7e6374">output_size</a> () const</td></tr>
<tr class="memdesc:a4a5e60cf35eb213d9d704d864d7e6374"><td class="mdescLeft">&#160;</td><td class="mdescRight">Output dimension of the last layer.  <a href="classcuda__mlp_1_1CudaNetwork.html#a4a5e60cf35eb213d9d704d864d7e6374">More...</a><br /></td></tr>
<tr class="separator:a4a5e60cf35eb213d9d704d864d7e6374"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acaaec6b927c6b5b4bdb927185aecd139"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#acaaec6b927c6b5b4bdb927185aecd139">params_data</a> ()</td></tr>
<tr class="memdesc:acaaec6b927c6b5b4bdb927185aecd139"><td class="mdescLeft">&#160;</td><td class="mdescRight">Mutable device pointer to parameters.  <a href="classcuda__mlp_1_1CudaNetwork.html#acaaec6b927c6b5b4bdb927185aecd139">More...</a><br /></td></tr>
<tr class="separator:acaaec6b927c6b5b4bdb927185aecd139"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abda6284e01f420bc74e00f211fcc2fc8"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#abda6284e01f420bc74e00f211fcc2fc8">grads_data</a> ()</td></tr>
<tr class="memdesc:abda6284e01f420bc74e00f211fcc2fc8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Mutable device pointer to gradients.  <a href="classcuda__mlp_1_1CudaNetwork.html#abda6284e01f420bc74e00f211fcc2fc8">More...</a><br /></td></tr>
<tr class="separator:abda6284e01f420bc74e00f211fcc2fc8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a51471ca3cd6a835b8e764f79e5c6ea48"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48">zeroGrads</a> ()</td></tr>
<tr class="memdesc:a51471ca3cd6a835b8e764f79e5c6ea48"><td class="mdescLeft">&#160;</td><td class="mdescRight">Zero all gradients.  <a href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48">More...</a><br /></td></tr>
<tr class="separator:a51471ca3cd6a835b8e764f79e5c6ea48"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a99a0434af96eeb351eaf799ab789d7"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a1a99a0434af96eeb351eaf799ab789d7">forward_only</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *input, int batch)</td></tr>
<tr class="memdesc:a1a99a0434af96eeb351eaf799ab789d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Forward pass only (no gradient computation)  <a href="classcuda__mlp_1_1CudaNetwork.html#a1a99a0434af96eeb351eaf799ab789d7">More...</a><br /></td></tr>
<tr class="separator:a1a99a0434af96eeb351eaf799ab789d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1348dfef5ae549707bea86ae88a0e02"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02">compute_loss_and_grad</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *input, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *target, int batch)</td></tr>
<tr class="memdesc:ab1348dfef5ae549707bea86ae88a0e02"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute MSE loss and gradients for a batch.  <a href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02">More...</a><br /></td></tr>
<tr class="separator:ab1348dfef5ae549707bea86ae88a0e02"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a384dbb4c92ca44c1d34bff21a9999de2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a384dbb4c92ca44c1d34bff21a9999de2">copy_output_to_host</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *host, size_t n) const</td></tr>
<tr class="memdesc:a384dbb4c92ca44c1d34bff21a9999de2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Copy the latest output activations to host memory.  <a href="classcuda__mlp_1_1CudaNetwork.html#a384dbb4c92ca44c1d34bff21a9999de2">More...</a><br /></td></tr>
<tr class="separator:a384dbb4c92ca44c1d34bff21a9999de2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a964c4ffde35500764a63227dfa4629ee"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html#a964c4ffde35500764a63227dfa4629ee">last_batch</a> () const</td></tr>
<tr class="memdesc:a964c4ffde35500764a63227dfa4629ee"><td class="mdescLeft">&#160;</td><td class="mdescRight">Batch size used in the last forward pass.  <a href="classcuda__mlp_1_1CudaNetwork.html#a964c4ffde35500764a63227dfa4629ee">More...</a><br /></td></tr>
<tr class="separator:a964c4ffde35500764a63227dfa4629ee"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Feed-forward dense network with GPU-backed parameters and gradients. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a81c0dd6e5d78f8dc822d4e1046da8258"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a81c0dd6e5d78f8dc822d4e1046da8258">&#9670;&nbsp;</a></span>CudaNetwork()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">cuda_mlp::CudaNetwork::CudaNetwork </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;&#160;</td>
          <td class="paramname"><em>handle</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Construct a network tied to a cuBLAS handle. </p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a6dfe3c5761ad5cb3293e1510eebdb971"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6dfe3c5761ad5cb3293e1510eebdb971">&#9670;&nbsp;</a></span>addLayer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::CudaNetwork::addLayer </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>in</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a>&#160;</td>
          <td class="paramname"><em>act</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Append a layer definition. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">in</td><td>Input dimension </td></tr>
    <tr><td class="paramname">out</td><td>Output dimension </td></tr>
    <tr><td class="paramname">act</td><td>Activation function </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aeb5df95bf3d986b04277996b6c32f9ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb5df95bf3d986b04277996b6c32f9ef">&#9670;&nbsp;</a></span>bindParams()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::CudaNetwork::bindParams </td>
          <td>(</td>
          <td class="paramtype">unsigned int&#160;</td>
          <td class="paramname"><em>seed</em> = <code>2341</code></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Allocate parameter/gradient buffers and initialize weights. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">seed</td><td>RNG seed for weight initialization </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_aeb5df95bf3d986b04277996b6c32f9ef_cgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_aeb5df95bf3d986b04277996b6c32f9ef_cgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_aeb5df95bf3d986b04277996b6c32f9ef_cgraph" id="aclasscuda__mlp_1_1CudaNetwork_aeb5df95bf3d986b04277996b6c32f9ef_cgraph">
<area shape="rect" title="Allocate parameter/gradient buffers and initialize weights." alt="" coords="5,149,187,191"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#ae303134f5eb2b62dbeafa95413eae7df" title="Copy from host to device, resizing as needed." alt="" coords="237,181,414,223"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a0494fd8a47d94eadca1d642cc92f4090" title="Resize the buffer, reallocating if needed." alt="" coords="475,240,652,281"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a22e12ab998f4a967adf2a4d7891abbf3" title="Mutable raw pointer to device memory." alt="" coords="475,5,652,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48" title="Zero all gradients." alt="" coords="235,91,416,132"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="715,189,884,215"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#aadeda0c95ac1348986fc0e030e2139bc" title="Free device memory, if allocated." alt="" coords="711,240,888,281"/>
<area shape="rect" href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5" title="Set device memory to zero." alt="" coords="464,71,663,98"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a7937ec6bd44bf48920e2c05e376c1c75" title="Current number of elements." alt="" coords="475,123,652,164"/>
</map>
</div>

</div>
</div>
<a id="ab1348dfef5ae549707bea86ae88a0e02"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab1348dfef5ae549707bea86ae88a0e02">&#9670;&nbsp;</a></span>compute_loss_and_grad()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> cuda_mlp::CudaNetwork::compute_loss_and_grad </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compute MSE loss and gradients for a batch. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>Input batch (in x batch) </td></tr>
    <tr><td class="paramname">target</td><td>Target batch (out x batch) </td></tr>
    <tr><td class="paramname">batch</td><td>Batch size </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Mean squared error loss </dd></dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_ab1348dfef5ae549707bea86ae88a0e02_cgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_ab1348dfef5ae549707bea86ae88a0e02_cgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_ab1348dfef5ae549707bea86ae88a0e02_cgraph" id="aclasscuda__mlp_1_1CudaNetwork_ab1348dfef5ae549707bea86ae88a0e02_cgraph">
<area shape="rect" title="Compute MSE loss and gradients for a batch." alt="" coords="5,137,192,178"/>
<area shape="rect" href="namespacecuda__mlp.html#a0f4e970087afb17ae397b2280d231961" title="Compute dot product on device using cuBLAS." alt="" coords="249,5,413,32"/>
<area shape="rect" href="namespacecuda__mlp.html#a3712a0feac483fd19d967d8f2e1ac6f8" title="Scale vector x &lt;&#45; alpha * x on device using cuBLAS." alt="" coords="246,56,415,83"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a1a99a0434af96eeb351eaf799ab789d7" title="Forward pass only (no gradient computation)" alt="" coords="240,107,421,149"/>
<area shape="rect" href="namespacecuda__mlp.html#af5b0352656cf797de01a1e71906318a8" title="Launch diff kernel." alt="" coords="493,148,657,175"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a4a5e60cf35eb213d9d704d864d7e6374" title="Output dimension of the last layer." alt="" coords="240,290,421,331"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48" title="Zero all gradients." alt="" coords="240,225,421,266"/>
<area shape="rect" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="485,5,665,32"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#abeb7f0750e956a5f7de086ee4274f17e" title="Access the raw cuBLAS handle." alt="" coords="469,56,681,83"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="729,173,899,200"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a22e12ab998f4a967adf2a4d7891abbf3" title="Mutable raw pointer to device memory." alt="" coords="487,250,664,291"/>
<area shape="rect" href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5" title="Set device memory to zero." alt="" coords="476,199,675,225"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a7937ec6bd44bf48920e2c05e376c1c75" title="Current number of elements." alt="" coords="487,315,664,357"/>
</map>
</div>

</div>
</div>
<a id="a384dbb4c92ca44c1d34bff21a9999de2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a384dbb4c92ca44c1d34bff21a9999de2">&#9670;&nbsp;</a></span>copy_output_to_host()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::CudaNetwork::copy_output_to_host </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>host</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Copy the latest output activations to host memory. </p>

</div>
</div>
<a id="a1a99a0434af96eeb351eaf799ab789d7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a99a0434af96eeb351eaf799ab789d7">&#9670;&nbsp;</a></span>forward_only()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::CudaNetwork::forward_only </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Forward pass only (no gradient computation) </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>Input batch (in x batch) </td></tr>
    <tr><td class="paramname">batch</td><td>Batch size </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_a1a99a0434af96eeb351eaf799ab789d7_icgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_a1a99a0434af96eeb351eaf799ab789d7_icgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_a1a99a0434af96eeb351eaf799ab789d7_icgraph" id="aclasscuda__mlp_1_1CudaNetwork_a1a99a0434af96eeb351eaf799ab789d7_icgraph">
<area shape="rect" title="Forward pass only (no gradient computation)" alt="" coords="240,5,421,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,5,192,47"/>
</map>
</div>

</div>
</div>
<a id="abda6284e01f420bc74e00f211fcc2fc8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abda6284e01f420bc74e00f211fcc2fc8">&#9670;&nbsp;</a></span>grads_data()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>* cuda_mlp::CudaNetwork::grads_data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Mutable device pointer to gradients. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_abda6284e01f420bc74e00f211fcc2fc8_cgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_abda6284e01f420bc74e00f211fcc2fc8_cgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_abda6284e01f420bc74e00f211fcc2fc8_cgraph" id="aclasscuda__mlp_1_1CudaNetwork_abda6284e01f420bc74e00f211fcc2fc8_cgraph">
<area shape="rect" title="Mutable device pointer to gradients." alt="" coords="5,5,187,47"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a22e12ab998f4a967adf2a4d7891abbf3" title="Mutable raw pointer to device memory." alt="" coords="235,5,412,47"/>
</map>
</div>

</div>
</div>
<a id="a964c4ffde35500764a63227dfa4629ee"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a964c4ffde35500764a63227dfa4629ee">&#9670;&nbsp;</a></span>last_batch()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int cuda_mlp::CudaNetwork::last_batch </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Batch size used in the last forward pass. </p>

</div>
</div>
<a id="a4a5e60cf35eb213d9d704d864d7e6374"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a5e60cf35eb213d9d704d864d7e6374">&#9670;&nbsp;</a></span>output_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">int cuda_mlp::CudaNetwork::output_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Output dimension of the last layer. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_a4a5e60cf35eb213d9d704d864d7e6374_icgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_a4a5e60cf35eb213d9d704d864d7e6374_icgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_a4a5e60cf35eb213d9d704d864d7e6374_icgraph" id="aclasscuda__mlp_1_1CudaNetwork_a4a5e60cf35eb213d9d704d864d7e6374_icgraph">
<area shape="rect" title="Output dimension of the last layer." alt="" coords="240,5,421,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,5,192,47"/>
</map>
</div>

</div>
</div>
<a id="acaaec6b927c6b5b4bdb927185aecd139"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acaaec6b927c6b5b4bdb927185aecd139">&#9670;&nbsp;</a></span>params_data()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>* cuda_mlp::CudaNetwork::params_data </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Mutable device pointer to parameters. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_acaaec6b927c6b5b4bdb927185aecd139_cgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_acaaec6b927c6b5b4bdb927185aecd139_cgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_acaaec6b927c6b5b4bdb927185aecd139_cgraph" id="aclasscuda__mlp_1_1CudaNetwork_acaaec6b927c6b5b4bdb927185aecd139_cgraph">
<area shape="rect" title="Mutable device pointer to parameters." alt="" coords="5,5,187,47"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a22e12ab998f4a967adf2a4d7891abbf3" title="Mutable raw pointer to device memory." alt="" coords="235,5,412,47"/>
</map>
</div>

</div>
</div>
<a id="a0ba4e8be8ba2f83046da28cd9a19830c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0ba4e8be8ba2f83046da28cd9a19830c">&#9670;&nbsp;</a></span>params_size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t cuda_mlp::CudaNetwork::params_size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Total number of parameters. </p>

</div>
</div>
<a id="a51471ca3cd6a835b8e764f79e5c6ea48"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a51471ca3cd6a835b8e764f79e5c6ea48">&#9670;&nbsp;</a></span>zeroGrads()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::CudaNetwork::zeroGrads </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Zero all gradients. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_cgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_cgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_cgraph" id="aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_cgraph">
<area shape="rect" title="Zero all gradients." alt="" coords="5,64,187,105"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a22e12ab998f4a967adf2a4d7891abbf3" title="Mutable raw pointer to device memory." alt="" coords="245,5,423,47"/>
<area shape="rect" href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5" title="Set device memory to zero." alt="" coords="235,71,433,98"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a7937ec6bd44bf48920e2c05e376c1c75" title="Current number of elements." alt="" coords="245,123,423,164"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="481,71,651,98"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_icgraph.png" border="0" usemap="#aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_icgraph" alt=""/></div>
<map name="aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_icgraph" id="aclasscuda__mlp_1_1CudaNetwork_a51471ca3cd6a835b8e764f79e5c6ea48_icgraph">
<area shape="rect" title="Zero all gradients." alt="" coords="240,39,421,80"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#aeb5df95bf3d986b04277996b6c32f9ef" title="Allocate parameter/gradient buffers and initialize weights." alt="" coords="8,5,189,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,71,192,112"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>src/cuda/<a class="el" href="network_8cuh.html">network.cuh</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: cuda_mlp Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">cuda_mlp Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">RAII-managed cuBLAS handle.  <a href="classcuda__mlp_1_1CublasHandle.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1DeviceBuffer.html">DeviceBuffer</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Owning buffer for device memory.  <a href="classcuda__mlp_1_1DeviceBuffer.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaGD.html">CudaGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gradient descent with optional momentum.  <a href="classcuda__mlp_1_1CudaGD.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structcuda__mlp_1_1IterationRecorder.html">IterationRecorder</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stores loss and gradient norm history on device.  <a href="structcuda__mlp_1_1IterationRecorder.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaDenseLayer.html">CudaDenseLayer</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fully connected layer with activation, using column-major matrices.  <a href="classcuda__mlp_1_1CudaDenseLayer.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaLBFGS.html">CudaLBFGS</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Limited-memory <a class="el" href="classBFGS.html" title="BFGS (Broyden–Fletcher–Goldfarb–Shanno) minimizer.">BFGS</a> with Armijo backtracking line search.  <a href="classcuda__mlp_1_1CudaLBFGS.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaMinimizerBase.html">CudaMinimizerBase</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract base class for CUDA-based minimizers.  <a href="classcuda__mlp_1_1CudaMinimizerBase.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaNetwork.html">CudaNetwork</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Feed-forward dense network with GPU-backed parameters and gradients.  <a href="classcuda__mlp_1_1CudaNetwork.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcuda__mlp_1_1CudaSGD.html">CudaSGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">SGD with optional momentum and learning-rate decay.  <a href="classcuda__mlp_1_1CudaSGD.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:a97b413e947c6702ad0157e46d429853e"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> = float</td></tr>
<tr class="memdesc:a97b413e947c6702ad0157e46d429853e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Scalar type used across CUDA kernels and optimizers.  <a href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">More...</a><br /></td></tr>
<tr class="separator:a97b413e947c6702ad0157e46d429853e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:add042d87fe3ba725b24f107b67e53742"><td class="memItemLeft" align="right" valign="top">enum class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a> : int { <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742a32a843da6ea40ab3b17a3421ccdf671b">Linear</a> = 0
, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742acc132a41cab5676334f353a22a0aa5c5">Tanh</a> = 1
, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742add10d919fa85cf27fc78c0e06fe0b378">ReLU</a> = 2
, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742a21eebb164e4b8b9bcf64fdb4d8d5dff4">Sigmoid</a> = 3
 }</td></tr>
<tr class="memdesc:add042d87fe3ba725b24f107b67e53742"><td class="mdescLeft">&#160;</td><td class="mdescRight">Supported activation functions.  <a href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">More...</a><br /></td></tr>
<tr class="separator:add042d87fe3ba725b24f107b67e53742"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ad0151301985ef1c9d60bc1a132c548d3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3">cuda_check</a> (cudaError_t err, const char *msg)</td></tr>
<tr class="memdesc:ad0151301985ef1c9d60bc1a132c548d3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Check a CUDA API call and abort with a message on failure.  <a href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3">More...</a><br /></td></tr>
<tr class="separator:ad0151301985ef1c9d60bc1a132c548d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a735f4a6c3c580edaad22de5975c66b58"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58">cublas_check</a> (cublasStatus_t status, const char *msg)</td></tr>
<tr class="memdesc:a735f4a6c3c580edaad22de5975c66b58"><td class="mdescLeft">&#160;</td><td class="mdescRight">Check a cuBLAS API call and abort with a message on failure.  <a href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58">More...</a><br /></td></tr>
<tr class="separator:a735f4a6c3c580edaad22de5975c66b58"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a86bc78b6dc35b5f0b80dcf5d110249c5"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5">device_set_zero</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *ptr, size_t n)</td></tr>
<tr class="memdesc:a86bc78b6dc35b5f0b80dcf5d110249c5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set device memory to zero.  <a href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5">More...</a><br /></td></tr>
<tr class="separator:a86bc78b6dc35b5f0b80dcf5d110249c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a215a45960999b21de6294e29d6d8a532"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a215a45960999b21de6294e29d6d8a532">device_copy</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *dst, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *src, size_t n)</td></tr>
<tr class="memdesc:a215a45960999b21de6294e29d6d8a532"><td class="mdescLeft">&#160;</td><td class="mdescRight">Copy device-to-device.  <a href="namespacecuda__mlp.html#a215a45960999b21de6294e29d6d8a532">More...</a><br /></td></tr>
<tr class="separator:a215a45960999b21de6294e29d6d8a532"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f4e970087afb17ae397b2280d231961"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a0f4e970087afb17ae397b2280d231961">device_dot</a> (<a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;handle, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *x, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *y, int n)</td></tr>
<tr class="memdesc:a0f4e970087afb17ae397b2280d231961"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute dot product on device using cuBLAS.  <a href="namespacecuda__mlp.html#a0f4e970087afb17ae397b2280d231961">More...</a><br /></td></tr>
<tr class="separator:a0f4e970087afb17ae397b2280d231961"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a51e81d68937e957ac0f2ab923657af35"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a51e81d68937e957ac0f2ab923657af35">device_nrm2</a> (<a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;handle, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *x, int n)</td></tr>
<tr class="memdesc:a51e81d68937e957ac0f2ab923657af35"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute Euclidean norm on device using cuBLAS.  <a href="namespacecuda__mlp.html#a51e81d68937e957ac0f2ab923657af35">More...</a><br /></td></tr>
<tr class="separator:a51e81d68937e957ac0f2ab923657af35"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af1c191bf546aef9e59039713a5e2d835"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#af1c191bf546aef9e59039713a5e2d835">device_axpy</a> (<a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;handle, int n, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> alpha, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *x, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *y)</td></tr>
<tr class="memdesc:af1c191bf546aef9e59039713a5e2d835"><td class="mdescLeft">&#160;</td><td class="mdescRight">y &lt;- alpha * x + y (AXPY) on device using cuBLAS.  <a href="namespacecuda__mlp.html#af1c191bf546aef9e59039713a5e2d835">More...</a><br /></td></tr>
<tr class="separator:af1c191bf546aef9e59039713a5e2d835"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3712a0feac483fd19d967d8f2e1ac6f8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a3712a0feac483fd19d967d8f2e1ac6f8">device_scal</a> (<a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;handle, int n, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> alpha, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *x)</td></tr>
<tr class="memdesc:a3712a0feac483fd19d967d8f2e1ac6f8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Scale vector x &lt;- alpha * x on device using cuBLAS.  <a href="namespacecuda__mlp.html#a3712a0feac483fd19d967d8f2e1ac6f8">More...</a><br /></td></tr>
<tr class="separator:a3712a0feac483fd19d967d8f2e1ac6f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6a3644353b56e2f50f80adcfc3f4ee00"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a6a3644353b56e2f50f80adcfc3f4ee00">activation_scale</a> (<a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a> act)</td></tr>
<tr class="memdesc:a6a3644353b56e2f50f80adcfc3f4ee00"><td class="mdescLeft">&#160;</td><td class="mdescRight">scaling factor for initialization.  <a href="namespacecuda__mlp.html#a6a3644353b56e2f50f80adcfc3f4ee00">More...</a><br /></td></tr>
<tr class="separator:a6a3644353b56e2f50f80adcfc3f4ee00"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a39ed9d7bc945a35120a283c58f18b234"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a39ed9d7bc945a35120a283c58f18b234">add_bias_kernel</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *z, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *b, int rows, int cols)</td></tr>
<tr class="memdesc:a39ed9d7bc945a35120a283c58f18b234"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: add bias vector to column-major matrix.  <a href="namespacecuda__mlp.html#a39ed9d7bc945a35120a283c58f18b234">More...</a><br /></td></tr>
<tr class="separator:a39ed9d7bc945a35120a283c58f18b234"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad23fdadabcb33621d301efb8f55061b4"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#ad23fdadabcb33621d301efb8f55061b4">activation_kernel</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *a, int n, int act)</td></tr>
<tr class="memdesc:ad23fdadabcb33621d301efb8f55061b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: apply activation in-place.  <a href="namespacecuda__mlp.html#ad23fdadabcb33621d301efb8f55061b4">More...</a><br /></td></tr>
<tr class="separator:ad23fdadabcb33621d301efb8f55061b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace5ac3a9a98695da4fe96eee9d7baf8d"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#ace5ac3a9a98695da4fe96eee9d7baf8d">activation_deriv_kernel</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *grad, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *a, int n, int act)</td></tr>
<tr class="memdesc:ace5ac3a9a98695da4fe96eee9d7baf8d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: multiply gradient by activation derivative.  <a href="namespacecuda__mlp.html#ace5ac3a9a98695da4fe96eee9d7baf8d">More...</a><br /></td></tr>
<tr class="separator:ace5ac3a9a98695da4fe96eee9d7baf8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a81abd70005964e3cb98ae99ff665f707"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a81abd70005964e3cb98ae99ff665f707">diff_kernel</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *output, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *target, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *diff, int n)</td></tr>
<tr class="memdesc:a81abd70005964e3cb98ae99ff665f707"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: diff = output - target.  <a href="namespacecuda__mlp.html#a81abd70005964e3cb98ae99ff665f707">More...</a><br /></td></tr>
<tr class="separator:a81abd70005964e3cb98ae99ff665f707"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afddab47a8654b1848c3030aef40a6634"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#afddab47a8654b1848c3030aef40a6634">sum_rows_kernel</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *mat, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *out, int rows, int cols)</td></tr>
<tr class="memdesc:afddab47a8654b1848c3030aef40a6634"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: sum columns (rows x cols) into a row vector.  <a href="namespacecuda__mlp.html#afddab47a8654b1848c3030aef40a6634">More...</a><br /></td></tr>
<tr class="separator:afddab47a8654b1848c3030aef40a6634"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abd24fc3c966441401f0f06926743e3e4"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#abd24fc3c966441401f0f06926743e3e4">launch_add_bias</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *z, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *b, int rows, int cols)</td></tr>
<tr class="memdesc:abd24fc3c966441401f0f06926743e3e4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch add-bias kernel.  <a href="namespacecuda__mlp.html#abd24fc3c966441401f0f06926743e3e4">More...</a><br /></td></tr>
<tr class="separator:abd24fc3c966441401f0f06926743e3e4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af291a8274e02d62ea667b6dd7c595bc6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#af291a8274e02d62ea667b6dd7c595bc6">launch_activation</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *a, int n, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a> act)</td></tr>
<tr class="memdesc:af291a8274e02d62ea667b6dd7c595bc6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch activation kernel.  <a href="namespacecuda__mlp.html#af291a8274e02d62ea667b6dd7c595bc6">More...</a><br /></td></tr>
<tr class="separator:af291a8274e02d62ea667b6dd7c595bc6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab07f4773848ee1516671610a538c8ae2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#ab07f4773848ee1516671610a538c8ae2">launch_activation_deriv</a> (<a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *grad, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *a, int n, <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a> act)</td></tr>
<tr class="memdesc:ab07f4773848ee1516671610a538c8ae2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch activation-derivative kernel.  <a href="namespacecuda__mlp.html#ab07f4773848ee1516671610a538c8ae2">More...</a><br /></td></tr>
<tr class="separator:ab07f4773848ee1516671610a538c8ae2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af5b0352656cf797de01a1e71906318a8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#af5b0352656cf797de01a1e71906318a8">launch_diff</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *output, const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *target, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *diff, int n)</td></tr>
<tr class="memdesc:af5b0352656cf797de01a1e71906318a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch diff kernel.  <a href="namespacecuda__mlp.html#af5b0352656cf797de01a1e71906318a8">More...</a><br /></td></tr>
<tr class="separator:af5b0352656cf797de01a1e71906318a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a171790bf97e28b10641775d966d496e0"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecuda__mlp.html#a171790bf97e28b10641775d966d496e0">launch_sum_rows</a> (const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *mat, <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *out, int rows, int cols)</td></tr>
<tr class="memdesc:a171790bf97e28b10641775d966d496e0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch sum-rows kernel.  <a href="namespacecuda__mlp.html#a171790bf97e28b10641775d966d496e0">More...</a><br /></td></tr>
<tr class="separator:a171790bf97e28b10641775d966d496e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="a97b413e947c6702ad0157e46d429853e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97b413e947c6702ad0157e46d429853e">&#9670;&nbsp;</a></span>CudaScalar</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">cuda_mlp::CudaScalar</a> = typedef float</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Scalar type used across CUDA kernels and optimizers. </p>

</div>
</div>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a id="add042d87fe3ba725b24f107b67e53742"></a>
<h2 class="memtitle"><span class="permalink"><a href="#add042d87fe3ba725b24f107b67e53742">&#9670;&nbsp;</a></span>ActivationType</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">cuda_mlp::ActivationType</a> : int</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Supported activation functions. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="add042d87fe3ba725b24f107b67e53742a32a843da6ea40ab3b17a3421ccdf671b"></a>Linear&#160;</td><td class="fielddoc"></td></tr>
<tr><td class="fieldname"><a id="add042d87fe3ba725b24f107b67e53742acc132a41cab5676334f353a22a0aa5c5"></a>Tanh&#160;</td><td class="fielddoc"></td></tr>
<tr><td class="fieldname"><a id="add042d87fe3ba725b24f107b67e53742add10d919fa85cf27fc78c0e06fe0b378"></a>ReLU&#160;</td><td class="fielddoc"></td></tr>
<tr><td class="fieldname"><a id="add042d87fe3ba725b24f107b67e53742a21eebb164e4b8b9bcf64fdb4d8d5dff4"></a>Sigmoid&#160;</td><td class="fielddoc"></td></tr>
</table>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="ace5ac3a9a98695da4fe96eee9d7baf8d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace5ac3a9a98695da4fe96eee9d7baf8d">&#9670;&nbsp;</a></span>activation_deriv_kernel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void cuda_mlp::activation_deriv_kernel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>grad</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>act</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: multiply gradient by activation derivative. </p>

</div>
</div>
<a id="ad23fdadabcb33621d301efb8f55061b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad23fdadabcb33621d301efb8f55061b4">&#9670;&nbsp;</a></span>activation_kernel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void cuda_mlp::activation_kernel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>act</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: apply activation in-place. </p>

</div>
</div>
<a id="a6a3644353b56e2f50f80adcfc3f4ee00"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6a3644353b56e2f50f80adcfc3f4ee00">&#9670;&nbsp;</a></span>activation_scale()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> cuda_mlp::activation_scale </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a>&#160;</td>
          <td class="paramname"><em>act</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>scaling factor for initialization. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a6a3644353b56e2f50f80adcfc3f4ee00_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a6a3644353b56e2f50f80adcfc3f4ee00_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a6a3644353b56e2f50f80adcfc3f4ee00_icgraph" id="anamespacecuda__mlp_a6a3644353b56e2f50f80adcfc3f4ee00_icgraph">
<area shape="rect" title="scaling factor for initialization." alt="" coords="256,5,415,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#af312d205604fe28f14391889ec836896" title="Recommended stddev for weight initialization." alt="" coords="5,5,208,47"/>
</map>
</div>

</div>
</div>
<a id="a39ed9d7bc945a35120a283c58f18b234"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a39ed9d7bc945a35120a283c58f18b234">&#9670;&nbsp;</a></span>add_bias_kernel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void cuda_mlp::add_bias_kernel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>b</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rows</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>cols</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: add bias vector to column-major matrix. </p>

</div>
</div>
<a id="a735f4a6c3c580edaad22de5975c66b58"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a735f4a6c3c580edaad22de5975c66b58">&#9670;&nbsp;</a></span>cublas_check()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::cublas_check </td>
          <td>(</td>
          <td class="paramtype">cublasStatus_t&#160;</td>
          <td class="paramname"><em>status</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Check a cuBLAS API call and abort with a message on failure. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">status</td><td>cuBLAS status code. </td></tr>
    <tr><td class="paramname">msg</td><td>Context string describing the operation. </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a735f4a6c3c580edaad22de5975c66b58_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a735f4a6c3c580edaad22de5975c66b58_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a735f4a6c3c580edaad22de5975c66b58_icgraph" id="anamespacecuda__mlp_a735f4a6c3c580edaad22de5975c66b58_icgraph">
<area shape="rect" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="493,187,673,214"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#a8859956e7919c0f3304fcb4438510e9a" title="Backward pass: compute dW, db, and optionally dX." alt="" coords="243,5,445,47"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#a3c41a1b0a015aaf79b7b7247d95fbe32" title="Construct and initialize the cuBLAS handle." alt="" coords="253,71,435,112"/>
<area shape="rect" href="namespacecuda__mlp.html#af1c191bf546aef9e59039713a5e2d835" title="y &lt;&#45; alpha * x + y (AXPY) on device using cuBLAS." alt="" coords="257,289,431,315"/>
<area shape="rect" href="namespacecuda__mlp.html#a0f4e970087afb17ae397b2280d231961" title="Compute dot product on device using cuBLAS." alt="" coords="262,137,426,163"/>
<area shape="rect" href="namespacecuda__mlp.html#a51e81d68937e957ac0f2ab923657af35" title="Compute Euclidean norm on device using cuBLAS." alt="" coords="255,238,433,265"/>
<area shape="rect" href="namespacecuda__mlp.html#a3712a0feac483fd19d967d8f2e1ac6f8" title="Scale vector x &lt;&#45; alpha * x on device using cuBLAS." alt="" coords="259,187,429,214"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#ab9377e28aecd00f8fa1b4ffc64ec42ff" title="Forward pass: Z = W*X + b, A = act(Z)" alt="" coords="243,340,445,381"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="16,176,184,217"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="5,242,195,269"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="19,293,181,335"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="7,111,193,152"/>
</map>
</div>

</div>
</div>
<a id="ad0151301985ef1c9d60bc1a132c548d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0151301985ef1c9d60bc1a132c548d3">&#9670;&nbsp;</a></span>cuda_check()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::cuda_check </td>
          <td>(</td>
          <td class="paramtype">cudaError_t&#160;</td>
          <td class="paramname"><em>err</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"><em>msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Check a CUDA API call and abort with a message on failure. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">err</td><td>CUDA error code returned by the runtime </td></tr>
    <tr><td class="paramname">msg</td><td>Context string describing the operation </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_ad0151301985ef1c9d60bc1a132c548d3_icgraph.png" border="0" usemap="#anamespacecuda__mlp_ad0151301985ef1c9d60bc1a132c548d3_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_ad0151301985ef1c9d60bc1a132c548d3_icgraph" id="anamespacecuda__mlp_ad0151301985ef1c9d60bc1a132c548d3_icgraph">
<area shape="rect" title="Check a CUDA API call and abort with a message on failure." alt="" coords="752,479,921,506"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#ae303134f5eb2b62dbeafa95413eae7df" title="Copy from host to device, resizing as needed." alt="" coords="255,5,432,47"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a0fd428a4132d244b37454b4f0a15e0ba" title="Copy from device to host." alt="" coords="511,611,688,652"/>
<area shape="rect" href="namespacecuda__mlp.html#a215a45960999b21de6294e29d6d8a532" title="Copy device&#45;to&#45;device." alt="" coords="512,559,687,586"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="259,501,427,543"/>
<area shape="rect" href="namespacecuda__mlp.html#a86bc78b6dc35b5f0b80dcf5d110249c5" title="Set device memory to zero." alt="" coords="500,341,699,367"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="249,305,438,331"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="262,421,425,463"/>
<area shape="rect" href="namespacecuda__mlp.html#af291a8274e02d62ea667b6dd7c595bc6" title="Launch activation kernel." alt="" coords="495,677,703,703"/>
<area shape="rect" href="namespacecuda__mlp.html#ab07f4773848ee1516671610a538c8ae2" title="Launch activation&#45;derivative kernel." alt="" coords="495,779,703,820"/>
<area shape="rect" href="namespacecuda__mlp.html#abd24fc3c966441401f0f06926743e3e4" title="Launch add&#45;bias kernel." alt="" coords="499,727,700,754"/>
<area shape="rect" href="namespacecuda__mlp.html#af5b0352656cf797de01a1e71906318a8" title="Launch diff kernel." alt="" coords="517,239,681,266"/>
<area shape="rect" href="namespacecuda__mlp.html#a171790bf97e28b10641775d966d496e0" title="Launch sum&#45;rows kernel." alt="" coords="495,845,704,871"/>
<area shape="rect" href="structcuda__mlp_1_1IterationRecorder.html#a61be3e9e7e83c0c80444c38a84e59707" title="Record a loss and gradient norm at an index." alt="" coords="496,443,703,484"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a0494fd8a47d94eadca1d642cc92f4090" title="Resize the buffer, reallocating if needed." alt="" coords="511,123,688,164"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#aeb5df95bf3d986b04277996b6c32f9ef" title="Allocate parameter/gradient buffers and initialize weights." alt="" coords="8,64,189,105"/>
<area shape="rect" href="structcuda__mlp_1_1IterationRecorder.html#a31658d32b8eca6e51d90284e6ac12228" title="Copy recorded values back to host vectors." alt="" coords="240,611,447,652"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48" title="Zero all gradients." alt="" coords="253,356,434,397"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,301,192,343"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#ab9377e28aecd00f8fa1b4ffc64ec42ff" title="Forward pass: Z = W*X + b, A = act(Z)" alt="" coords="242,704,445,745"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#a8859956e7919c0f3304fcb4438510e9a" title="Backward pass: compute dW, db, and optionally dX." alt="" coords="242,795,445,836"/>
<area shape="rect" href="classcuda__mlp_1_1DeviceBuffer.html#a2b1bd9a5d6d7d797873d92a960388296" title="Construct and allocate a buffer of size n." alt="" coords="255,123,432,164"/>
<area shape="rect" href="structcuda__mlp_1_1IterationRecorder.html#a2818ccb4b3d556f89c694acc0f63d237" title="Allocate buffers for up to capacity iterations." alt="" coords="240,188,447,229"/>
</map>
</div>

</div>
</div>
<a id="af1c191bf546aef9e59039713a5e2d835"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af1c191bf546aef9e59039713a5e2d835">&#9670;&nbsp;</a></span>device_axpy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::device_axpy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;&#160;</td>
          <td class="paramname"><em>handle</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>y &lt;- alpha * x + y (AXPY) on device using cuBLAS. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_cgraph.png" border="0" usemap="#anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_cgraph" id="anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_cgraph">
<area shape="rect" title="y &lt;&#45; alpha * x + y (AXPY) on device using cuBLAS." alt="" coords="5,31,180,57"/>
<area shape="rect" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="244,5,424,32"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#abeb7f0750e956a5f7de086ee4274f17e" title="Access the raw cuBLAS handle." alt="" coords="228,56,440,83"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_icgraph.png" border="0" usemap="#anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_icgraph" id="anamespacecuda__mlp_af1c191bf546aef9e59039713a5e2d835_icgraph">
<area shape="rect" title="y &lt;&#45; alpha * x + y (AXPY) on device using cuBLAS." alt="" coords="243,71,417,98"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="16,5,184,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="5,71,195,98"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="19,123,181,164"/>
</map>
</div>

</div>
</div>
<a id="a215a45960999b21de6294e29d6d8a532"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a215a45960999b21de6294e29d6d8a532">&#9670;&nbsp;</a></span>device_copy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::device_copy </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>dst</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>src</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Copy device-to-device. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">dst</td><td>Destination device pointer. </td></tr>
    <tr><td class="paramname">src</td><td>Source device pointer. </td></tr>
    <tr><td class="paramname">n</td><td>Number of elements. </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_cgraph" id="anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_cgraph">
<area shape="rect" title="Copy device&#45;to&#45;device." alt="" coords="5,5,180,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="228,5,397,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_icgraph" id="anamespacecuda__mlp_a215a45960999b21de6294e29d6d8a532_icgraph">
<area shape="rect" title="Copy device&#45;to&#45;device." alt="" coords="221,13,396,39"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="5,5,173,47"/>
</map>
</div>

</div>
</div>
<a id="a0f4e970087afb17ae397b2280d231961"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0f4e970087afb17ae397b2280d231961">&#9670;&nbsp;</a></span>device_dot()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> cuda_mlp::device_dot </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;&#160;</td>
          <td class="paramname"><em>handle</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compute dot product on device using cuBLAS. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_cgraph" id="anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_cgraph">
<area shape="rect" title="Compute dot product on device using cuBLAS." alt="" coords="5,31,169,57"/>
<area shape="rect" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="233,5,413,32"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#abeb7f0750e956a5f7de086ee4274f17e" title="Access the raw cuBLAS handle." alt="" coords="217,56,429,83"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_icgraph" id="anamespacecuda__mlp_a0f4e970087afb17ae397b2280d231961_icgraph">
<area shape="rect" title="Compute dot product on device using cuBLAS." alt="" coords="240,46,404,73"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,5,192,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="15,71,183,112"/>
</map>
</div>

</div>
</div>
<a id="a51e81d68937e957ac0f2ab923657af35"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a51e81d68937e957ac0f2ab923657af35">&#9670;&nbsp;</a></span>device_nrm2()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> cuda_mlp::device_nrm2 </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;&#160;</td>
          <td class="paramname"><em>handle</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compute Euclidean norm on device using cuBLAS. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_cgraph" id="anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_cgraph">
<area shape="rect" title="Compute Euclidean norm on device using cuBLAS." alt="" coords="5,31,183,57"/>
<area shape="rect" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="247,5,427,32"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#abeb7f0750e956a5f7de086ee4274f17e" title="Access the raw cuBLAS handle." alt="" coords="231,56,443,83"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_icgraph" id="anamespacecuda__mlp_a51e81d68937e957ac0f2ab923657af35_icgraph">
<area shape="rect" title="Compute Euclidean norm on device using cuBLAS." alt="" coords="243,71,420,98"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="16,5,184,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="5,71,195,98"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="19,123,181,164"/>
</map>
</div>

</div>
</div>
<a id="a3712a0feac483fd19d967d8f2e1ac6f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3712a0feac483fd19d967d8f2e1ac6f8">&#9670;&nbsp;</a></span>device_scal()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::device_scal </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classcuda__mlp_1_1CublasHandle.html">CublasHandle</a> &amp;&#160;</td>
          <td class="paramname"><em>handle</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a>&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>x</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Scale vector x &lt;- alpha * x on device using cuBLAS. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_cgraph" id="anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_cgraph">
<area shape="rect" title="Scale vector x &lt;&#45; alpha * x on device using cuBLAS." alt="" coords="5,31,175,57"/>
<area shape="rect" href="namespacecuda__mlp.html#a735f4a6c3c580edaad22de5975c66b58" title="Check a cuBLAS API call and abort with a message on failure." alt="" coords="239,5,419,32"/>
<area shape="rect" href="classcuda__mlp_1_1CublasHandle.html#abeb7f0750e956a5f7de086ee4274f17e" title="Access the raw cuBLAS handle." alt="" coords="223,56,435,83"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_icgraph" id="anamespacecuda__mlp_a3712a0feac483fd19d967d8f2e1ac6f8_icgraph">
<area shape="rect" title="Scale vector x &lt;&#45; alpha * x on device using cuBLAS." alt="" coords="243,107,412,134"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="7,5,193,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaLBFGS.html#a91a3611d456c2958b236125b444aec3f" title="Run L&#45;BFGS optimization." alt="" coords="16,71,184,112"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="5,137,195,163"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="19,188,181,229"/>
</map>
</div>

</div>
</div>
<a id="a86bc78b6dc35b5f0b80dcf5d110249c5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a86bc78b6dc35b5f0b80dcf5d110249c5">&#9670;&nbsp;</a></span>device_set_zero()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::device_set_zero </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>ptr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set device memory to zero. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">ptr</td><td>Device pointer. </td></tr>
    <tr><td class="paramname">n</td><td>Number of elements. </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_cgraph" id="anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_cgraph">
<area shape="rect" title="Set device memory to zero." alt="" coords="5,5,204,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="252,5,421,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_icgraph" id="anamespacecuda__mlp_a86bc78b6dc35b5f0b80dcf5d110249c5_icgraph">
<area shape="rect" title="Set device memory to zero." alt="" coords="477,64,676,91"/>
<area shape="rect" href="classcuda__mlp_1_1CudaGD.html#af4f49873129a98cea837fa7284448d9a" title="Run full&#45;batch gradient descent." alt="" coords="240,5,429,32"/>
<area shape="rect" href="classcuda__mlp_1_1CudaSGD.html#a87c5b29a57e0361ca6186edcc7eb0dd7" title="Run SGD optimization." alt="" coords="253,57,416,98"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#a51471ca3cd6a835b8e764f79e5c6ea48" title="Zero all gradients." alt="" coords="244,122,425,163"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#aeb5df95bf3d986b04277996b6c32f9ef" title="Allocate parameter/gradient buffers and initialize weights." alt="" coords="8,89,189,130"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,154,192,195"/>
</map>
</div>

</div>
</div>
<a id="a81abd70005964e3cb98ae99ff665f707"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a81abd70005964e3cb98ae99ff665f707">&#9670;&nbsp;</a></span>diff_kernel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void cuda_mlp::diff_kernel </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>diff</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: diff = output - target. </p>

</div>
</div>
<a id="af291a8274e02d62ea667b6dd7c595bc6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af291a8274e02d62ea667b6dd7c595bc6">&#9670;&nbsp;</a></span>launch_activation()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::launch_activation </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a>&#160;</td>
          <td class="paramname"><em>act</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Launch activation kernel. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_cgraph.png" border="0" usemap="#anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_cgraph" id="anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_cgraph">
<area shape="rect" title="Launch activation kernel." alt="" coords="5,5,213,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="261,5,431,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_icgraph.png" border="0" usemap="#anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_icgraph" id="anamespacecuda__mlp_af291a8274e02d62ea667b6dd7c595bc6_icgraph">
<area shape="rect" title="Launch activation kernel." alt="" coords="256,13,464,39"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#ab9377e28aecd00f8fa1b4ffc64ec42ff" title="Forward pass: Z = W*X + b, A = act(Z)" alt="" coords="5,5,208,47"/>
</map>
</div>

</div>
</div>
<a id="ab07f4773848ee1516671610a538c8ae2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab07f4773848ee1516671610a538c8ae2">&#9670;&nbsp;</a></span>launch_activation_deriv()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::launch_activation_deriv </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>grad</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#add042d87fe3ba725b24f107b67e53742">ActivationType</a>&#160;</td>
          <td class="paramname"><em>act</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Launch activation-derivative kernel. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_cgraph.png" border="0" usemap="#anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_cgraph" id="anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_cgraph">
<area shape="rect" title="Launch activation&#45;derivative kernel." alt="" coords="5,5,213,47"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="261,13,431,39"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_icgraph.png" border="0" usemap="#anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_icgraph" id="anamespacecuda__mlp_ab07f4773848ee1516671610a538c8ae2_icgraph">
<area shape="rect" title="Launch activation&#45;derivative kernel." alt="" coords="256,5,464,47"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#a8859956e7919c0f3304fcb4438510e9a" title="Backward pass: compute dW, db, and optionally dX." alt="" coords="5,5,208,47"/>
</map>
</div>

</div>
</div>
<a id="abd24fc3c966441401f0f06926743e3e4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abd24fc3c966441401f0f06926743e3e4">&#9670;&nbsp;</a></span>launch_add_bias()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::launch_add_bias </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>b</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rows</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>cols</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Launch add-bias kernel. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_cgraph.png" border="0" usemap="#anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_cgraph" id="anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_cgraph">
<area shape="rect" title="Launch add&#45;bias kernel." alt="" coords="5,5,207,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="255,5,424,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_icgraph.png" border="0" usemap="#anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_icgraph" id="anamespacecuda__mlp_abd24fc3c966441401f0f06926743e3e4_icgraph">
<area shape="rect" title="Launch add&#45;bias kernel." alt="" coords="256,13,457,39"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#ab9377e28aecd00f8fa1b4ffc64ec42ff" title="Forward pass: Z = W*X + b, A = act(Z)" alt="" coords="5,5,208,47"/>
</map>
</div>

</div>
</div>
<a id="af5b0352656cf797de01a1e71906318a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af5b0352656cf797de01a1e71906318a8">&#9670;&nbsp;</a></span>launch_diff()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::launch_diff </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>diff</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Launch diff kernel. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_cgraph.png" border="0" usemap="#anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_cgraph" id="anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_cgraph">
<area shape="rect" title="Launch diff kernel." alt="" coords="5,5,169,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="217,5,387,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_icgraph.png" border="0" usemap="#anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_icgraph" id="anamespacecuda__mlp_af5b0352656cf797de01a1e71906318a8_icgraph">
<area shape="rect" title="Launch diff kernel." alt="" coords="240,13,404,39"/>
<area shape="rect" href="classcuda__mlp_1_1CudaNetwork.html#ab1348dfef5ae549707bea86ae88a0e02" title="Compute MSE loss and gradients for a batch." alt="" coords="5,5,192,47"/>
</map>
</div>

</div>
</div>
<a id="a171790bf97e28b10641775d966d496e0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a171790bf97e28b10641775d966d496e0">&#9670;&nbsp;</a></span>launch_sum_rows()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_mlp::launch_sum_rows </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>mat</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rows</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>cols</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Launch sum-rows kernel. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a171790bf97e28b10641775d966d496e0_cgraph.png" border="0" usemap="#anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_cgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_cgraph" id="anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_cgraph">
<area shape="rect" title="Launch sum&#45;rows kernel." alt="" coords="5,5,215,32"/>
<area shape="rect" href="namespacecuda__mlp.html#ad0151301985ef1c9d60bc1a132c548d3" title="Check a CUDA API call and abort with a message on failure." alt="" coords="263,5,432,32"/>
</map>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="namespacecuda__mlp_a171790bf97e28b10641775d966d496e0_icgraph.png" border="0" usemap="#anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_icgraph" alt=""/></div>
<map name="anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_icgraph" id="anamespacecuda__mlp_a171790bf97e28b10641775d966d496e0_icgraph">
<area shape="rect" title="Launch sum&#45;rows kernel." alt="" coords="256,13,465,39"/>
<area shape="rect" href="classcuda__mlp_1_1CudaDenseLayer.html#a8859956e7919c0f3304fcb4438510e9a" title="Backward pass: compute dW, db, and optionally dX." alt="" coords="5,5,208,47"/>
</map>
</div>

</div>
</div>
<a id="afddab47a8654b1848c3030aef40a6634"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afddab47a8654b1848c3030aef40a6634">&#9670;&nbsp;</a></span>sum_rows_kernel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void cuda_mlp::sum_rows_kernel </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>mat</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecuda__mlp.html#a97b413e947c6702ad0157e46d429853e">CudaScalar</a> *&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rows</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>cols</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: sum columns (rows x cols) into a row vector. </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>

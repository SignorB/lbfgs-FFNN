<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>My Project: L-BFGS and Stochastic L-BFGS for Optimization</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">My Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">L-BFGS and Stochastic L-BFGS for Optimization </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> This project implements advanced Quasi-Newton optimization methods, specifically <b>L-BFGS</b> (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) and its stochastic variant <b>S-LBFGS</b>, designed for large-scale and finite-sum minimization problems.</p>
<h1><a class="anchor" id="autotoc_md1"></a>
Problem Statement</h1>
<p>We consider the classical unconstrained non-linear optimization problem:</p>
<p>$$ \min_{w \in \mathbb{R}^d} F(w), $$</p>
<p>where $F : \mathbb{R}^d \to \mathbb{R}$ is a smooth objective function.</p>
<h2><a class="anchor" id="autotoc_md2"></a>
Deterministic Optimization (L-BFGS)</h2>
<p>In the standard setting, we assume we can evaluate $F(w)$ and its full gradient $\nabla F(w)$ precisely. A standard second-order method is <b>Newton’s method</b>:</p>
<p>$$ w_{k+1} = w_k - H_k^{-1} \nabla F(w_k), $$</p>
<p>where $H_k = \nabla^2 F(w_k)$ is the Hessian. While <a class="el" href="classNewton.html" title="Newton minimizer (full Newton) for unconstrained optimization.">Newton</a>'s method has quadratic convergence, it is computationally expensive ($O(d^3)$ or $O(d^2)$ per step). <b>L-BFGS</b> approximates the product $H_k^{-1} v$ using a history of the last $m$ updates, reducing memory complexity to $O(md)$ and time complexity to $O(md)$.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Stochastic Optimization (S-LBFGS)</h2>
<p>We specifically address the <b>Finite Sum Minimization</b> problem, which is central to machine learning and neural network training:</p>
<p>$$ F(w) = \frac{1}{N} \sum_{i=1}^N f_i(w), $$</p>
<p>where $N$ is the number of data points (or component functions), and $f_i(w)$ calculates the loss for the $i$-th sample (e.g., squared error $f_i(w) = \frac{1}{2}|h(x_i; w) - y_i|^2$).</p>
<p>For large $N$, computing the full gradient $\nabla F(w) = \frac{1}{N} \sum \nabla f_i(w)$ at every iteration is prohibitively expensive. <b>Stochastic Gradient Descent (SGD)</b> addresses this by using a mini-batch $S \subset 1, \dots, N </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
